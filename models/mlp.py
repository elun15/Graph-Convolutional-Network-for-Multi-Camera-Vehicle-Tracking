from torch import nn


class MLP(nn.Module):
    def __init__(self, input_dim, fc_dims, dropout_p=0.4, use_batchnorm=False, is_classifier=False):
        super(MLP, self).__init__()

        assert isinstance(fc_dims, (list, tuple)), 'fc_dims must be either a list or a tuple, but got {}'.format(
            type(fc_dims))

        layers = []
        if not(is_classifier):
            for dim in fc_dims:
                layers.append(nn.Linear(input_dim, dim))
                if use_batchnorm and dim != 1:
                    layers.append(nn.BatchNorm1d(dim,track_running_stats=False))

                if dim != 1:
                    layers.append(nn.ReLU(inplace=True))

                if dropout_p is not None and dim != 1:
                    layers.append(nn.Dropout(p=dropout_p))

                input_dim = dim
        else:
            for dim in fc_dims:
                layers.append(nn.Linear(input_dim, dim))


        self.fc_layers = nn.Sequential(*layers)

    def forward(self, input):
        return self.fc_layers(input)


